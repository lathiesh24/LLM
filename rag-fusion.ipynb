{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai qdrant-client langchain_pinecone \n",
    "%pip install --upgrade --quiet  langchain-pinecone langchain-openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = CSVLoader(r\"C:\\Users\\lathu\\Desktop\\LLM\\TYN_STARTUP_DATA - Final Data (1).csv\",encoding=\"latin-1\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "   )\n",
    "chunks1 = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://7b2c647e-0f04-4ed5-95c9-a1801529452a.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "api_key = \"lmlafpaSSQTAtbMYxm-os9kdVL7iaQIA__CrvDdOLzJ7AV3VFfwXtA\"\n",
    "vectorstore1= Qdrant.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=url,\n",
    "    prefer_grpc=True,\n",
    "    api_key=api_key,\n",
    "    collection_name=\"docs1_chunk\",\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore1.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template=\"\\n       You are a Q&A Expert tasked with assisting users in finding startup solutions for their technical problems. \\n       \\n       Your primary role is to extract relevant startup IDs and names from the provided context based on the user's problem statement, represented as {input}. \\n       \\n       Understand the user's question, which outlines a specific problem, and convert it into a technical requirement. Analyze the startups available in the {context} that can address the problem technically, and provide a list of startup names that offer potential solutions.\\n\\n       Your response should consist of a comma-separated list of startup names, adhering to the format: `'Startup Name 1', 'Startup Name 2', 'Startup Name 3', 'Startup Name 4', 'Startup Name 5', ...`. Note that you may list multiple startup names if there are several viable solutions present in the {context}. Avoid generating new companies; only include those mentioned or available in the provided context.\\n\\n       Please proceed to assist the user by generating a list of startup names that could address their technical problem.\\n\")), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='You are a helpful assistant that generates multiple search queries based on a single input query. \\n\\nGenerate multiple search queries related to: {input} \\n\\nOutput (4 queries):')), FewShotChatMessagePromptTemplate(examples=[{'input': \"I need a platform capable of managing 'n' number of clouds in a single platform, and migration should be simple to do\", 'output': 'Abiquo, Dobiquo'}, {'input': 'Give me list of startups leveraging AI in Quantum computing', 'output': '1Qbit'}, {'input': 'I am facing a problem with complex search, and I need a platform to enhance my search', 'output': 'AlogliA'}, {'input': 'In my organization, if anything happens, I need to recover the data.', 'output': 'Actifio, Doctifio'}, {'input': 'Give me startups that provide innovative services to empower IoT initiatives across diverse industries, ensuring seamless integration', 'output': 'Actility, Doctility'}, {'input': 'I am facing access control challenges with full manual processes. I need a startup that can help me', 'output': 'AlertEnterprise'}, {'input': 'I need to monitor & track my containers in the ship yards', 'output': 'Arviem'}], example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], template='{output}'))])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the custom RAG prompt template\n",
    "custom_rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"\n",
    "       You are a Q&A Expert tasked with assisting users in solutions  \n",
    "       \n",
    "       Your primary role is to extract relevant answer  from the provided context based on the user, represented as {input}. \n",
    "       \n",
    "       Understand the user's question, which outlines a specific problem. \n",
    "\"\"\"),\n",
    "              (\"system\",\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {input} \\n\n",
    "Output (4 queries):\"\"\"), \n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "custom_rag_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I see that your message is empty. Please let me know how I can assist you further.')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
